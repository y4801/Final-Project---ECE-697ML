{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxLYVAZLHrq9"
   },
   "source": [
    "#  Comparison of CNN to Vision Transformer model trained on HiRISE Mars Satellite Images     (Feel free to change this)\n",
    "### by Aniruddha Prasad and Andrew Hartnett\n",
    "\n",
    "The following notebook will compare the accuracies of a Convolutional Neural Network (CNN) and Vision Transformer (ViT) trained on satellite images taken of Mars from the HiRISE dataset. The goal of this work is to determine whether or not a pre-trained ViT model, which has been seen used as the state-of-the-art for image classification in certain circumstances, will prove better when pre-trained on a significant size dataset and fine-tuned to this data. Then, we will train 3 version of each model with larger and larger subsets of the data to determine the trend in accuracy for each model. This will tell us which model will be best as more images are accumulated over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Prepare the Training Data\n",
    "2. Define and Train the CNN - **WIP**\n",
    "3. Define and Train the Vision Transformer (ViT) - **WIP**\n",
    "4. Evaluate CNN vs ViT - **WIP**\n",
    "5. Retrain CNN and ViT on small, medium, and full HiRISE - **WIP**\n",
    "6. Compare three CNNs vs three ViTs - **WIP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g91PowrTJY8Z"
   },
   "source": [
    "## 1. Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tendorflow imports\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('PS') #prevent import error due to venv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "# Imports for dataset separation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Improve progress bar display\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "tqdm.tqdm = tqdm.auto.tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The way Neihusst Preprocesses their data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = []\n",
    "data_labels = []\n",
    "rel_img_path = 'hirise-map-proj-v3/map-proj/' # add path of folder to image name for later loading\n",
    "\n",
    "# open up the labeled data file\n",
    "with open('hirise-map-proj-v3/labels-map-proj.txt') as labels:\n",
    "  for line in labels:\n",
    "    file_name, label = line.split(' ')\n",
    "    data_images.append(rel_img_path + file_name)\n",
    "    data_labels.append(int(label))\n",
    "\n",
    "# divide data into testing and training (total len 3820)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    data_images, data_labels, test_size=0.15, random_state=666)\n",
    "test_len = len(test_images)   # 573\n",
    "train_len = len(train_images) # 3247\n",
    "\n",
    "# label translations\n",
    "class_labels = ['other','crater','dark_dune','streak',\n",
    "                'bright_dune','impact','edge']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3247\n"
     ]
    }
   ],
   "source": [
    "# Print length of training set (should be 3247)\n",
    "print(train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n"
     ]
    }
   ],
   "source": [
    "# Print length of testing set (should be 573)\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert image paths into numpy matrices\n",
    "def parse_image(filename):\n",
    "  img_obj = Image.open(filename)\n",
    "  img = np.asarray(img_obj).astype(np.float32)\n",
    "  #normalize image to 0-1 range\n",
    "  img /= 255.0\n",
    "  return img\n",
    "\n",
    "train_images = np.array(list(map(parse_image, train_images)))\n",
    "test_images = np.array(list(map(parse_image, test_images)))\n",
    "\n",
    "# Add 4th dimension to image arrays to allow for model.fit to take them as inputs\n",
    "train_images = np.reshape(train_images, (-1, 227, 227, 1))\n",
    "test_images = np.reshape(test_images, (-1, 227, 227, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 227, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that training images have been created into a 3D array of 3247 images of 227x227 pixels\n",
    "np.shape(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.4       ]\n",
      "  [0.4       ]\n",
      "  [0.40392157]\n",
      "  ...\n",
      "  [0.32156864]\n",
      "  [0.30980393]\n",
      "  [0.29803923]]\n",
      "\n",
      " [[0.4       ]\n",
      "  [0.4       ]\n",
      "  [0.4       ]\n",
      "  ...\n",
      "  [0.30588236]\n",
      "  [0.2901961 ]\n",
      "  [0.27058825]]\n",
      "\n",
      " [[0.4       ]\n",
      "  [0.4       ]\n",
      "  [0.39607844]\n",
      "  ...\n",
      "  [0.29803923]\n",
      "  [0.26666668]\n",
      "  [0.23921569]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.4117647 ]\n",
      "  [0.4117647 ]\n",
      "  [0.4117647 ]\n",
      "  ...\n",
      "  [0.654902  ]\n",
      "  [0.6862745 ]\n",
      "  [0.6784314 ]]\n",
      "\n",
      " [[0.40392157]\n",
      "  [0.40392157]\n",
      "  [0.40784314]\n",
      "  ...\n",
      "  [0.70980394]\n",
      "  [0.75686276]\n",
      "  [0.76862746]]\n",
      "\n",
      " [[0.4       ]\n",
      "  [0.4       ]\n",
      "  [0.4       ]\n",
      "  ...\n",
      "  [0.74509805]\n",
      "  [0.80784315]\n",
      "  [0.8352941 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(label):\n",
    "  encoding = [0 for _ in range(len(class_labels))]\n",
    "  encoding[label] = 1\n",
    "  return np.array(encoding).astype(np.float32)\n",
    "\n",
    "train_labels = np.array(list(map(to_one_hot, train_labels)))\n",
    "test_labels = np.array(list(map(to_one_hot, test_labels)))\n",
    "\n",
    "train_labels = np.reshape(train_labels, (-1, 7))\n",
    "test_labels = np.reshape(test_labels, (-1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print one-hot encoding of labels (train_image[0] is classified as \"dark_dune\")\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5s4SiueJdS0"
   },
   "source": [
    "## 2. Define and Train the CNN - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Source: https://github.com/niehusst/HiRISE-Net\n",
    "\n",
    "This project by **niehusst** on GitHub sought to emulate the HiRISENet model used in the \"Deep Mars\" paper. The CNN defined in his project provided us a base CNN to use which we tuned to improve the test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xijbkS3HHpF6"
   },
   "outputs": [],
   "source": [
    "# make a generator to train the model with\n",
    "generator = ImageDataGenerator(rotation_range=0, zoom_range=0,\n",
    "    width_shift_range=0, height_shift_range=0, shear_range=0,\n",
    "    horizontal_flip=False, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 227, 227, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 113, 113, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 113, 113, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               25690240  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 25,709,959\n",
      "Trainable params: 25,709,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###             BUILD SHAPE OF THE MODEL              ###\n",
    "\n",
    "# increase kernel size and stride??\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "      input_shape=(227,227,1)),\n",
    "  tf.keras.layers.MaxPooling2D((2,2), strides=2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "  tf.keras.layers.MaxPooling2D((2,2), strides=2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(7, activation=tf.nn.softmax), # final layer with node for each classification\n",
    "])\n",
    "\n",
    "# specify loss and SGD functions\n",
    "model.compile(optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 102 steps\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 81s 795ms/step - loss: 1.0654 - accuracy: 0.6840\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 84s 826ms/step - loss: 0.6654 - accuracy: 0.7666\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 89s 875ms/step - loss: 0.4765 - accuracy: 0.8281\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 87s 851ms/step - loss: 0.3276 - accuracy: 0.8873\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 87s 854ms/step - loss: 0.2301 - accuracy: 0.9242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x203364ece08>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###                 TRAIN THE MODEL                   ###\n",
    "\n",
    "#specify training metadata\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# train the model on the training data\n",
    "num_epochs = 5\n",
    "model.fit(generator.flow(train_images, train_labels, batch_size=BATCH_SIZE), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd9islKCJqR7"
   },
   "source": [
    "## 3. Define and Train the Vision Transformer (ViT) - WIP\n",
    "Website used as a source: https://theaisummer.com/hugging-face-vit/\n",
    "\n",
    "Should change this code with this: https://github.com/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files from repository.\n",
    "\n",
    "import sys\n",
    "if './vision_transformer' not in sys.path:\n",
    "  sys.path.append('./vision_transformer')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vit_jax import checkpoint\n",
    "from vit_jax import models\n",
    "from vit_jax import train\n",
    "from vit_jax.configs import augreg as augreg_config\n",
    "from vit_jax.configs import models as models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISREGARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBR\n",
    "from datasets import load_dataset\n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "\n",
    "# TBR\n",
    "splits - train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training metric to minimize\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Instantiate ViT model\n",
    "from transformers import ViTForImageClassification\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    \n",
    "    images = examples['img']\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "    \n",
    "    return examples\n",
    "\n",
    "from datasets import Features, ClassLabel, Array3D\n",
    "\n",
    "features = Features({\n",
    "    'label': ClassLabels(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 244)),\n",
    "})\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator - Used for forming batches from the dataset when training the model\n",
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - Part 1\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-ink21k')\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model - Part 2\n",
    "from transformers import ViTModel\n",
    "\n",
    "class ViTForImageClassification2(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_labels=10):\n",
    "        \n",
    "        super(ViTForImageClassification2, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def forward(self, pixel_values, labels):\n",
    "        \n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(output)\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics during evaluation (CUSTOM - May need to change)\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predicitons=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = preprocessed_train_ds,\n",
    "    eval_dataset = preprocessed_val_ds,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    \"test-cifar-10\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks - This cell is not complete\n",
    "from transformers import WandbCallback\n",
    "callbacks = [WandbCallback(...)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate CNN vs ViT - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - 3s 5ms/sample - loss: 0.6363 - accuracy: 0.7871\n",
      "Final loss was 0.6362588601170618.\n",
      "Accuracy of model was 0.7870855331420898\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the CNN - BEST IS 81% AFTER 5 EPOCHS\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Final loss was {}.\\nAccuracy of model was {}\".format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the ViT\n",
    "outputs = trainer.predict(preprocessed_test_ds)\n",
    "y_pred = outputs.predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot for the clout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRjTKI3ULWfZ"
   },
   "source": [
    "#### Short response to our findings:\n",
    "Was the output expected? what did we do for optimizations? is it overfit/underfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrain CNN and ViT on small, medium, and full HiRISE - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare three CNNs vs three ViTs - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for the clout\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MarsLookLikeMoon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
